{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7102e25",
   "metadata": {},
   "source": [
    "# Expectation-Maximization (EM) for parameter inference and hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c38ca",
   "metadata": {},
   "source": [
    "### Defining the likelihood \n",
    "\n",
    "A central goal of modeling is to predict the distribution of possible outcomes that could occur under a given experimental condition. A data driven approach for developing a model is to use previously collected experimental data to train a model to perform this task. Measurements from individual experimental conditions are almost always assumed to be independent, meaning that observing the outcomes from an experimental condition will not influence the outcome of an other experimental condition. Experimental measurements are imprecise due to measurement noise, and because there are typically many sources of noise, the central limit theorem justifies the assumption that measurement noise will be normally distributed. The choice of distribution to characterize noise is called the *noise model*. We use a different model, sometimes called the *central model*, to predict the expected value of an outcome from a particular experimental condition and characterize the noise using the noise model, \n",
    "\n",
    "\\begin{equation}\n",
    "    y(q_i) = f(q_i, \\theta) + \\varepsilon\n",
    "    \\label{eq: model}\n",
    "\\end{equation}\n",
    "where $q_i$ denotes a particular experimental condition, $y(q_i)$ denotes the measurement from the experimental condition,  $f(q_i, \\theta)$ is the model prediction of the measurement, $\\theta$ is a vector of model parameters, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is a random variable sampled from the noise model. The central model and noise model together define the sampling distribution,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y(q_i) | q_i, \\theta, \\sigma^2) = \\mathcal{N}(y(q_i) | f(q_i, \\theta), \\sigma^2),\n",
    "    \\label{eq: sampling distribution}\n",
    "\\end{equation}\n",
    "which quantifies how likely a measurement was sampled from the predicted conditional distribution given the experimental condition, $q_i$. \n",
    "An experimental design is a set of experimental conditions, $\\mathbf{q} = \\{q_1, ..., q_n \\}$, and data collected from an experimental design is denoted as $\\mathcal{D}(\\mathbf{q}) = \\{y(q_1), ..., y(q_n) \\}$. Because measurements from each experimental condition are assumed to be drawn independently from the sampling distribution, the likelihood of a set of measurements is\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathcal{D}(\\mathbf{q}) | \\mathbf{q}, \\theta, \\sigma^2) = \\prod_{i=1}^{n} \\mathcal{N}(y(q_i) | f(q_i, \\theta), \\sigma^2).\n",
    "\\end{equation}\n",
    "One approach for parameter estimation is to maximize the log of the likelihood function with respect to parameters, \n",
    "\n",
    "\\begin{align}\n",
    "    \\theta_{MLE} &= \\underset{\\theta}{\\text{argmax}} \\; \\text{log} \\; p(\\mathcal{D}(\\mathbf{q}) | \\theta, \\sigma^2) \\nonumber \\\\ &= \\underset{\\theta}{\\text{argmax}} \\; \\sum_{i=1}^n \\left( - \\frac{(y(q_i) - f(q_i, \\theta))^2}{2\\sigma^2} - \\text{log} \\; \\sigma \\right).\n",
    "\\end{align}\n",
    "A limitation of the maximum likelihood approach is that it does not allow for the incorporation of prior knowledge about the parameters, which in the simplest case can be useful for promoting parsimonious parameter estimates by setting the mean of the parameter prior to zero. Furthermore, incorporating a prior enables a Bayesian approach to infer a posterior parameter distribution, which is useful for understanding how well parameters are constrained by data, quantifying model prediction uncertainty, model selection, and designing informative experiments that can be used to further refine the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcef530",
   "metadata": {},
   "source": [
    "### Variational Inference and Expectation-Maximization for parameter inference and hyper-parameter optimization\n",
    "\n",
    "The goal of Bayesian parameter inference is to compute a posterior parameter distribution conditioned on all available data, denoted as $p(\\theta | \\mathcal{D}(\\mathbf{q}))$. Computing a posterior distribution analytically is only possible for models that are linear with respect to parameters; however, posterior distributions can be approximated by either sampling from the distribution using methods like Markov chain Monte Carlo (MCMC) or by optimizing the parameters, denoted as $\\phi$, of an approximate probability density function, $z(\\theta | \\phi) \\approx p(\\theta | \\mathcal{D}(\\mathbf{q}))$. We will use an independent Gaussian prior distribution for each parameter, $p(\\theta_k | \\alpha_k) = \\mathcal{N}(\\theta_k | 0, 1/ \\alpha_k)$, where $\\alpha_k$ is the precision (inverse variance) of $\\theta_k$.    \n",
    "Measurement noise is also assumed to be Gaussian with variance, $\\sigma^2$. The set of variables, $\\xi = \\{\\alpha_1, ..., \\alpha_{n_\\theta}, \\sigma^{2} \\}$, are considered hyper-parameters, because they influence parameter inference but are distinct from the model parameters, $\\theta$. The objective function that takes into account model parameters and hyper-parameters is the log marginal likelihood given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{log} \\; p (\\mathcal{D}(\\mathbf{q}) | \\xi) = \\text{log} \\; \\int_{\\theta} p (\\mathcal{D}(\\mathbf{q}), \\theta | \\xi)  d \\theta.\n",
    "\\end{equation}\n",
    "For any choice of $z(\\theta | \\phi)$, the log marginal likelihood can be decomposed into two functions, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{log} \\; p (\\mathcal{D}(\\mathbf{q}) | \\xi) = \n",
    "    \\underbrace{\\int_{\\theta} \\text{log} \\left( \\frac{p(\\mathcal{D}(\\mathbf{q}), \\theta | \\xi)}{z(\\theta | \\phi)} \\right) z(\\theta | \\phi) d \\theta}_{\\mathcal{L}(z(\\theta | \\phi), \\xi)} + \n",
    "    \\underbrace{\\int_{\\theta} -\\text{log} \\left( \\frac{p(\\theta | \\mathcal{D}(\\mathbf{q}), \\xi)}{z(\\theta | \\phi)} \\right) z(\\theta | \\phi) d \\theta}_{\\text{KL}}.\n",
    "\\end{equation}\n",
    "where $\\text{KL}$ is the Kullback-Leibler divergence between the approximating distribution $z(\\theta | \\phi)$ and the true posterior parameter distribution $p(\\theta | \\mathcal{D}(\\mathbf{q}), \\xi)$. Because the KL divergence is strictly non-negative, the function $\\mathcal{L}(z(\\theta | \\phi), \\xi)$ is a lower bound on the log marginal likelihood since $\\mathcal{L}(z(\\theta | \\phi), \\xi) \\leq \\text{log} \\; p (\\mathcal{D}(\\mathbf{q}) | \\xi)$. This lower bound is referred to as the evidence lower bound (ELBO). Optimizing the ELBO with respect to the parameters of the approximate posterior distribution, $\\phi$, is called variational inference. The Expectation-Maximization algorithm iterates between an expectation step, which involves variational inference to maximize the ELBO with respect to $\\phi$ keeping $\\xi$ fixed, followed by a maximization step, in which the ELBO is maximized with respect to $\\xi$ keeping $\\phi$ fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0adc24",
   "metadata": {},
   "source": [
    "### Expectation step\n",
    "\n",
    "The expectiation step involves first optimizing the parameters of the approximate posterior distribution, $\\phi$, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi^* = \\underset{\\phi}{\\text{argmax}} \\; \\mathcal{L}(z(\\theta | \\phi), \\xi)\n",
    "\\end{equation}\n",
    "and then evaluating the expectation using the optimized parameters, \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(z(\\theta | \\phi^*), \\xi) &= \\int_{\\theta} \\text{log} \\left( \\frac{p(\\mathcal{D}(\\mathbf{q}), \\theta | \\xi)}{z(\\theta | \\phi^*)} \\right) z(\\theta | \\phi^*) d \\theta \\nonumber \\\\ \n",
    "    &= \\mathbb{E}_{z | \\phi^*} \\left[ \\text{log} \\left( \\frac{p(\\mathcal{D}(\\mathbf{q}), \\theta | \\xi)}{z(\\theta | \\phi^*)} \\right) \\right]\n",
    "    \\label{eq: ELBO}\n",
    "\\end{align}\n",
    "Using a Gaussian approximation of the posterior parameter distribution, the variational parameters are the mean and covariance, $z(\\theta | \\phi) = \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma})$. Evaluating the ELBO (keeping only terms that depend on the variational parameters) gives, \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(\\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}), \\xi) &= \\int_{\\theta} \\text{log} \\left( \\frac{p(\\mathcal{D}(\\mathbf{q}), \\theta | \\xi)} {\\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma})} \\right) \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\ \n",
    "    &= \\int_{\\theta} \\text{log} \\left( p(\\mathcal{D}(\\mathbf{q}) | \\theta, \\sigma^2) \\right) \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\  \n",
    "    &\\quad + \\int_{\\theta} \\text{log} \\left( \\mathcal{N}(\\theta | \\mathbf{0}, \\text{diag}(1/\\mathbf{\\alpha})) \\right) \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\ \n",
    "    &\\quad - \\int_{\\theta} \\text{log} \\left( \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) \\right) \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\ \n",
    "    &= \\int_{\\theta} \\sum_{i=1}^n \\left( - \\frac{(y(q_i) - f(q_i, \\theta))^2}{2 \\sigma^2} - \\text{log} \\; \\sigma \\right) \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\\n",
    "    &\\quad + \\sum_{k=1}^{n_{\\theta}} \\left( -\\frac{1}{2} \\alpha_k \\mu_k^2 -\\frac{1}{2} \\alpha_k \\Sigma_{kk} + \\frac{1}{2} \\text{log} \\; \\alpha_k \\right) \\nonumber \\\\ \n",
    "    &\\quad + \\frac{1}{2} \\; \\text{log} \\; \\text{det} \\; \\mathbf{\\Sigma} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e63e8",
   "metadata": {},
   "source": [
    "For a linear regression model, the parameters are the regression coefficients that map a basis function of the input to the output, \n",
    "\n",
    "\\begin{equation}\n",
    "f(q_i, \\theta) := \\theta ^T \\cdot \\phi(q_i) = \\sum_{j=1}^{m} \\theta_j \\cdot \\phi_j(q_i)\n",
    "\\end{equation}\n",
    "where $\\phi(q_i) \\in \\mathbb{R}^m$ is a basis function that operates on the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc8a57",
   "metadata": {},
   "source": [
    "Evaluating the expectation with the linear model gives, \n",
    "\n",
    "\\begin{align}\n",
    "     &\\int_{\\theta} \\sum_{i=1}^n \\left( - \\frac{(y(q_i) - \\theta^T \\cdot \\phi(q_i))^2}{2 \\sigma^2} - \\text{log} \\; \\sigma \\right) \\mathcal{N}(\\theta | \\mu,   \\mathbf{\\Sigma}) d \\theta \\nonumber \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed50db8",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "     &= \\sum_{i=1}^n \\left( -\\frac{1}{2 \\sigma^2} \\int_{\\theta} (y(q_i) - \\theta^T \\cdot \\phi(q_i))^2 \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta - \\text{log} \\; \\sigma \\right) \\nonumber \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0661691",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "     &= -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left( \\int_{\\theta} (y(q_i) - \\theta^T \\cdot \\phi(q_i))^2 \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\right)  - n \\; \\text{log} \\; \\sigma \\nonumber \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1852527",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "     &= -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left( y^2(q_i) -2 y(q_i) \\mu^T \\cdot \\phi(q_i) + \\int_{\\theta} (\\theta^T \\phi(q_i))^2 \\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}) d \\theta \\right)  - n \\; \\text{log} \\; \\sigma \\nonumber \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ee5cc",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "     &= -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left( y^2(q_i) -2 y(q_i) \\mu^T \\cdot \\phi(q_i) + (\\mu^T \\phi(q_i))^2 + \\phi(q_i)^T \\Sigma \\phi(q_i) \\right)  - n \\; \\text{log} \\; \\sigma \\nonumber \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcbbed",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "     &= -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left( (y(q_i) - \\mu^T \\cdot \\phi(q_i) )^2 + \\phi(q_i)^T \\Sigma \\phi(q_i) \\right)  - n \\; \\text{log} \\; \\sigma \\nonumber \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90baafd1",
   "metadata": {},
   "source": [
    "Putting all of the terms together gives, \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(\\mathcal{N}(\\theta | \\mu, \\mathbf{\\Sigma}), \\xi) &= -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n \\left( (y(q_i) - \\mu^T \\cdot \\phi(q_i) )^2 + \\phi(q_i)^T \\Sigma \\phi(q_i) \\right)  - n \\; \\text{log} \\; \\sigma \\nonumber \\\\ \n",
    "    &\\quad + \\sum_{k=1}^{n_{\\theta}} \\left( -\\frac{1}{2} \\alpha_k \\mu_k^2 -\\frac{1}{2} \\alpha_k \\Sigma_{kk} + \\frac{1}{2} \\text{log} \\; \\alpha_k \\right) \\nonumber \\\\ \n",
    "    &\\quad + \\frac{1}{2} \\; \\text{log} \\; \\text{det} \\; \\mathbf{\\Sigma}\n",
    "\\label{eq: approximate ELBO}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1da07",
   "metadata": {},
   "source": [
    "Maximizing the ELBO with respect to $\\mathbf{\\Sigma}$ gives, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Sigma}^* = \\left( \\mathrm{diag}(\\alpha) + \\frac{1}{\\sigma^2} \\Phi^T \\Phi \\right)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "Maximizing the expression for the ELBO with respect to $\\mu$ gives, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mu^* = \\frac{1}{\\sigma^2} \\mathbf{\\Sigma}^* \\Phi^T \\mathbf{y}\n",
    "\\end{equation}\n",
    "where $\\Phi_{ij} = \\phi_j(q_i)$ and $\\mathbf{y} = (y_1, ..., y_n)^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e49ce",
   "metadata": {},
   "source": [
    "### Maximization step\n",
    "\n",
    "The maximization step involves maximizing the ELBO where $\\mu = \\mu^*$ and $\\mathbf{\\Sigma} = \\mathbf{\\Sigma}^*$ with respect to each $\\alpha_k$ and $\\sigma^2$, which gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha_k^* = \\frac{1}{\\mu_k^{*2} + \\Sigma_{kk}^*}\n",
    "\\end{equation}\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma^{*2} = \\frac{1}{n} \\sum_{i=1}^n (y(q_i) - \\mu^{*T} \\phi(q_i))^2 + \\phi(q_i)^T \\mathbf{\\Sigma}^* \\phi(q_i)\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
